
.. blogpost::
    :title: Build onnxruntime on WSL (Windows Linux Subsystem)
    :keywords: build
    :date: 2021-12-16
    :categories: build
    :lid: blog-build-wsl-2021

    I tried to build :epkg:`onnxruntime-training` for GPU
    on :epkg:`WSL` (Windows Linux Subsystem). I took
    the distribution `Ubuntu 20.04`. Paths should be updated
    according to your installation.
    
    **some useful commands once installed**

    ::

        nvidia-smi
        nsys

    Let's assume WSL is installed, otherwise, here are some useful commands.

    ::

        # see all local distributions
        wsl -s -l
        
        # see available distributions online
        wsl --list --online
        
        # install one distribution
        wsl --install -d Ubuntu-20.04

    
    Installation of required packages.

    ::

        sudo apt-get install cmake
        sudo apt-get install zlib1g-dev
        sudo apt-get install libssl-dev
        sudo apt-get install python3-dev

    Installation of :epkg:`cmake`.

    ::

        curl -OL https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1.tar.gz
        tar -zxvf cmake-3.22.1.tar.gz
        cd cmake-3.22.1
        ./bootstrap
        make 
        sudo make install
        export PATH=~/install/cmake-3.22.1/bin/:$PATH
        
    Installation of :epkg:`openmpi`:

    ::

        gunzip -c openmpi-4.1.2.tar.gz | tar xf -
        cd openmpi-4.1.2
        ./configure --prefix=/usr/local --with_cuda
        make all install

    Installation of CUDA (choose a compatible version with :epkg:`pytorch`, 11.3 for example).

    See ` <https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch03-running-cuda>`_

    ::

        wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
        sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
        wget https://developer.download.nvidia.com/compute/cuda/11.3.0/local_installers/cuda-repo-wsl-ubuntu-11-3-local_11.3.0-1_amd64.deb
        sudo dpkg -i cuda-repo-wsl-ubuntu-11-3-local_11.3.0-1_amd64.deb
        sudo apt-key add /var/cuda-repo-wsl-ubuntu-11-3-local/7fa2af80.pub
        sudo apt-get update
        sudo apt-get -y install cuda

    Installation of :epkg:`cudnn` (after it is downloaded):

    ::

        sudo dpkg -i cudnn-local-repo-ubuntu2004-8.3.1.22_1.0-1_amd64.deb
        sudo apt-key add /var/cudnn-local-repo-*/7fa2af80.pub
        sudo apt-get update
        sudo apt-get install libcudnn8
        sudo apt-get install libcudnn8-dev
        
    Installation of :epkg:`nccl`
    
    See `Install NCCL <https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html>`_.

    ::

        sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/7fa2af80.pub
        sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"
        sudo apt update
        sudo apt install libnccl2 libnccl-dev

    Installation of :epkg:`pytorch`:

    ::

        python3 -m pip install torch==1.10.1+cu113 torchvision==0.11.2+cu113 torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html

    Then to check CUDA is available:

    ::

        import torch
        print(torch.cuda.is_available())

    Build :epkg:`onnxruntime-training`:

    ::

        export CUDACXX=/usr/local/cuda-11.3/bin/nvcc
        alias python=python3
        python3 ./tools/ci_build/build.py --config RelWithDebInfo --enable_training --use_cuda --cuda_home /usr/local/cuda-11.3/ --cudnn_home /usr/local/cuda-11.3/ --build_wheel --parallel --cuda_version=11.3 --skip_tests --enable_training_torch_interop --build_dir ./build/linux_gpu
        

    Another option is to use a docker:
    `Running Existing GPU Accelerated Containers on WSL 2
    <https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch05-running-containers>`_.

    
