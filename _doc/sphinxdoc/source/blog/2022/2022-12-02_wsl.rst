
.. blogpost::
    :title: Build onnxruntime on WSL (Windows Linux Subsystem)
    :keywords: build
    :date: 2022-12-02
    :categories: build
    :lid: blog-build-wsl-2022

    I tried to build :epkg:`onnxruntime-training` for GPU
    on :epkg:`WSL` (Windows Linux Subsystem). I took
    the distribution `Ubuntu 20.04`. Paths should be updated
    according to your installation.

    **some useful commands once installed**

    ::

        nvidia-smi
        nsys

    Let's assume WSL is installed, otherwise, here are some useful commands.

    ::

        # see all local distributions
        wsl -s -l

        # see available distributions online
        wsl --list --online

        # install one distribution or download it
        wsl --install -d Ubuntu-22.04

    Installation of required packages.

    ::

        sudo apt-get install -y cmake zlib1g-dev libssl-dev python3-dev libhwloc-dev libevent-dev libcurl4-openssl-dev

    Let's install :epkg:`gcc`:

    ::

        sudo apt-get update && sudo apt-get upgrade -y
        sudo apt autoremove -y
        sudo apt install libcurl4 ca-certificates --reinstall
        sudo apt-get install gcc g++ -y
        gcc --version

    Installation of :epkg:`cmake`.

    ::

        mkdir install
        cd install
        curl -OL https://github.com/Kitware/CMake/releases/download/v3.25.1/cmake-3.25.1.tar.gz
        tar -zxvf cmake-3.25.1.tar.gz
        cd cmake-3.25.1
        ./bootstrap --system-curl
        make
        sudo make install
        export PATH=~/install/cmake-3.25.1/bin/:$PATH
        cmake --version

    Installation of :epkg:`openmpi`:

    ::

        sudo apt-get install openmpi-bin

    Installation of CUDA (choose a compatible version with :epkg:`pytorch`, 11.7 for example).

    See `CUDA on WSL User Guide
    <https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch03-running-cuda>`_

    ::

        export CUDA_VERSION=11.7
        export CUDA_VERSION_=11-7
        wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin
        sudo mv cuda-wsl-ubuntu.pin /etc/apt/preferences.d/cuda-repository-pin-600
        wget https://developer.download.nvidia.com/compute/cuda/${CUDA_VERSION}.0/local_installers/cuda-repo-wsl-ubuntu-${CUDA_VERSION_}-local_${CUDA_VERSION}.0-1_amd64.deb
        sudo dpkg -i cuda-repo-wsl-ubuntu-${CUDA_VERSION_}-local_${CUDA_VERSION}.0-1_amd64.deb
        sudo cp /var/cuda-repo-wsl-ubuntu-11-7-local/cuda-B81839D3-keyring.gpg /usr/share/keyrings/ 
        sudo apt-get update
        sudo apt-get -y install cuda

    Now you may run `nvidia-smi -L` to list the available GPUs.

    Installation of :epkg:`cudnn` (after it is downloaded):

    ::

        sudo dpkg -i cudnn-local-repo-ubuntu2204-8.7.0.84_1.0-1_amd64.deb
        sudo cp /var/cudnn-local-repo-ubuntu2204-8.7.0.84/cudnn-local-BF23AD8A-keyring.gpg /usr/share/keyrings/
        sudo apt-get update
        sudo apt-get install libcudnn8  libcudnn8-dev

    Installation of :epkg:`nccl`

    See `Install NCCL <https://docs.nvidia.com/deeplearning/nccl/install-guide/index.html>`_.

    ::

        sudo dpkg -i nccl-local-repo-ubuntu2204-2.15.5-cuda11.8_1.0-1_amd64.deb
        sudo cp /var/nccl-local-repo-ubuntu2204-2.15.5-cuda11.8/nccl-local-1F5D0FB9-keyring.gpg /usr/share/keyrings/
        sudo add-apt-repository "deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /"
        sudo apt update
        sudo apt install libnccl2 libnccl-dev

    Installation of pip and update python packages:

    ::

        sudo apt-get install -y protobuf-compiler libprotobuf-dev python3-pybind11 libpython3.10-dev
        https://bootstrap.pypa.io/get-pip.py
        python3 get-pip.py
        python3 -m pip install --upgrade numpy jupyter pandas statsmodels scipy scikit-learn pybind11 cython protobuf flatbuffers

    Installation of :epkg:`pytorch`:

    ::

        python3 -m pip install torch torchvision torchaudio

    Then to check CUDA is available:

    ::

        import torch
        print(torch.cuda.is_available())

    Install openmpi:

    ::

        wget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.4.tar.gz
        gunzip -c openmpi-4.1.4.tar.gz | tar xf -
        cd openmpi-4.1.4
        ./configure --prefix=/usr/local --with-cuda
        make all install

    To build onnx from source (`python -m pip install onnx` also works),
    protobuf is too recent for onnx, it needs to be build from an older version.

    ::

        cd ..
        git clone https://github.com/protocolbuffers/protobuf.git
        cd protobuf
        git checkout v3.11.3
        git submodule update --init --recursive
        mkdir build_source && cd build_source
        cmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release
        make -j2
        make install
        protoc --version

    Then onnx built inplace:

    ::

        git clone https://github.com/onnx/onnx.git
        cd onnx
        python setup.py build
        python setup.py build_ext --inplace

    Build :epkg:`onnxruntime-training`:

    ::

        alias python=python3
        export CUDA_VERSION=11.7
        export CUDACXX=/usr/local/cuda-${CUDA_VERSION}/bin/nvcc
        # export MPI_HOME=~/install/openmpi-4.1.2
        python3 ./tools/ci_build/build.py --skip_tests --build_dir ./build/linux_gpu --config Release --use_mpi false --enable_training --enable_training_torch_interop --use_cuda --cuda_version=${CUDA_VERSION} --cuda_home /usr/local/cuda-${CUDA_VERSION}/ --cudnn_home /usr/local/cuda-${CUDA_VERSION}/ --build_wheel --parallel

    Option ``--parallel 1`` can be used to fix the parallelism while building onnxruntime.
    Option `--use_mpi false` can be replaced by `--mpi_home /usr/local/lib/openmpi`.

    Another option is to use a docker:
    `Running Existing GPU Accelerated Containers on WSL 2
    <https://docs.nvidia.com/cuda/wsl-user-guide/index.html#ch05-running-containers>`_.




























